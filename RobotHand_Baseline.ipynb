{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM1WQyD6fzDp",
        "outputId": "da2f4d95-3416-4833-8514-b7c8f4196008"
      },
      "outputs": [],
      "source": [
        "! pip install gymnasium\n",
        "! pip install gymnasium-robotics\n",
        "! pip install mujoco\n",
        "! pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pj56axYCf2Qa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "from tensorboardX import SummaryWriter\n",
        "import os\n",
        "from datetime import datetime\n",
        "import gymnasium_robotics\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_GMLySWpf6cN"
      },
      "outputs": [],
      "source": [
        "# Register robotics environments\n",
        "gym.register_envs(gymnasium_robotics)\n",
        "\n",
        "# Set device for PyTorch (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jyDCVUQQf84U"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    \"\"\"\n",
        "    Actor Network Architecture with LSTM for handling temporal dependencies.\n",
        "    This network maps states to a probability distribution over discretized actions.\n",
        "\n",
        "    Architecture:\n",
        "    1. Fully connected layer for initial feature extraction\n",
        "    2. LSTM layer for temporal dependency modeling\n",
        "    3. Output layers that produce discrete probability distributions for each action dimension\n",
        "\n",
        "    The action space is discretized into 11 values between -1 and 1 to make the continuous\n",
        "    control problem more tractable while maintaining fine-grained control.\n",
        "\n",
        "    Args:\n",
        "        obs_dim (int): Dimension of the observation/state space\n",
        "        action_dim (int): Dimension of the action space\n",
        "        fc_hidden_dim (int, optional): Hidden size of fully connected layer. Defaults to 256\n",
        "        lstm_hidden_dim (int, optional): Hidden size of LSTM layer. Defaults to 128\n",
        "    \"\"\"\n",
        "    def __init__(self, obs_dim, action_dim, fc_hidden_dim=256, lstm_hidden_dim=128):\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        # Initial feature extraction\n",
        "        self.fc1 = nn.Linear(obs_dim, fc_hidden_dim)\n",
        "\n",
        "        # Temporal dependency modeling\n",
        "        self.lstm_layer = nn.LSTM(fc_hidden_dim, lstm_hidden_dim, batch_first=True)\n",
        "\n",
        "        # Action distribution generation\n",
        "        self.output_sequential = torch.nn.Sequential(\n",
        "            nn.Linear(lstm_hidden_dim, action_dim * 11),  # 11 discrete values per action dimension\n",
        "            nn.Unflatten(dim=-1, unflattened_size=(action_dim, 11)),  # Reshape to (action_dim, 11)\n",
        "            nn.Softmax(dim=-1)  # Convert to probability distribution\n",
        "        )\n",
        "\n",
        "        # Define discretized action values from -1 to 1\n",
        "        self.action_tensor = torch.tensor(\n",
        "            [-1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1]\n",
        "        ).to(device)\n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input state/observation\n",
        "            hidden_state (tuple, optional): LSTM hidden state and cell state.\n",
        "                                          If None, initialized as zeros.\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - action_prob (torch.Tensor): Action probability distributions\n",
        "                - hidden_state (tuple): Updated LSTM hidden state and cell state\n",
        "        \"\"\"\n",
        "        # Feature extraction\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        # LSTM processing\n",
        "        if hidden_state is None:\n",
        "            x, hidden_state = self.lstm_layer(x)  # Initialize hidden state as zeros\n",
        "        else:\n",
        "            x, hidden_state = self.lstm_layer(x, hidden_state)  # Use provided hidden state\n",
        "\n",
        "        # Generate action probabilities\n",
        "        action_prob = self.output_sequential(x)\n",
        "\n",
        "        return action_prob, hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FEfQDbUyf_uF"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    \"\"\"\n",
        "    Critic Network Architecture with LSTM for handling temporal dependencies.\n",
        "    This network estimates the value function for given states.\n",
        "\n",
        "    Architecture:\n",
        "    1. Fully connected layer for initial feature extraction\n",
        "    2. LSTM layer for temporal dependency modeling\n",
        "    3. Output layer producing scalar state values\n",
        "\n",
        "    Args:\n",
        "        obs_dim (int): Dimension of the observation/state space\n",
        "        fc_hidden_dim (int, optional): Hidden size of fully connected layer. Defaults to 256\n",
        "        lstm_hidden_dim (int, optional): Hidden size of LSTM layer. Defaults to 128\n",
        "    \"\"\"\n",
        "    def __init__(self, obs_dim, fc_hidden_dim=256, lstm_hidden_dim=128):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # Initial feature extraction\n",
        "        self.fc1 = nn.Linear(obs_dim, fc_hidden_dim)\n",
        "\n",
        "        # Temporal dependency modeling\n",
        "        self.lstm_layer = nn.LSTM(fc_hidden_dim, lstm_hidden_dim, batch_first=True)\n",
        "\n",
        "        # Value prediction\n",
        "        self.output_layer = nn.Linear(lstm_hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x, hidden_state=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input state/observation\n",
        "            hidden_state (tuple, optional): LSTM hidden state and cell state.\n",
        "                                          If None, initialized as zeros.\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - values (torch.Tensor): Predicted state values\n",
        "                - hidden_state (tuple): Updated LSTM hidden state and cell state\n",
        "        \"\"\"\n",
        "        # Feature extraction\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        # LSTM processing with automatic hidden state initialization\n",
        "        if hidden_state is None:\n",
        "            x, hidden_state = self.lstm_layer(x)\n",
        "        else:\n",
        "            x, hidden_state = self.lstm_layer(x, hidden_state)\n",
        "\n",
        "        # Generate value predictions\n",
        "        values = self.output_layer(x)\n",
        "\n",
        "        return values, hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "N_ReE5bOgG9i"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "    \"\"\"\n",
        "    Fixed-size circular buffer for storing and managing experience replay data.\n",
        "    Implements a FIFO (First-In-First-Out) queue when buffer is full.\n",
        "\n",
        "    This buffer stores:\n",
        "    - observations: State observations from the environment\n",
        "    - actions: Actions taken by the agent\n",
        "    - rewards: Rewards received from the environment\n",
        "    - values: Value function estimates from the critic\n",
        "    - dones: Episode termination flags\n",
        "    - log_probs: Log probabilities of taken actions\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): Maximum size of the buffer\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size):\n",
        "        # Initialize empty lists for storing experiences\n",
        "        self.observations = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.values = []\n",
        "        self.dones = []\n",
        "        self.log_probs = []\n",
        "        self.batch_size = batch_size\n",
        "        self.current_size = 0\n",
        "\n",
        "    def add_memo(self, observation, action, reward, value, done, log_prob=None):\n",
        "        \"\"\"\n",
        "        Add a new experience to the buffer. If buffer is full, remove oldest experience.\n",
        "\n",
        "        Args:\n",
        "            observation (np.ndarray): State observation\n",
        "            action (np.ndarray): Action taken\n",
        "            reward (float): Reward received\n",
        "            value (float): Value estimate\n",
        "            done (bool): Episode termination flag\n",
        "            log_prob (np.ndarray, optional): Log probability of the action\n",
        "        \"\"\"\n",
        "        # If buffer is full, remove oldest entries (FIFO)\n",
        "        if len(self.observations) >= self.batch_size:\n",
        "            self.observations.pop(0)\n",
        "            self.actions.pop(0)\n",
        "            self.rewards.pop(0)\n",
        "            self.values.pop(0)\n",
        "            self.dones.pop(0)\n",
        "            if log_prob is not None:\n",
        "                self.log_probs.pop(0)\n",
        "\n",
        "        # Add new experience\n",
        "        self.observations.append(observation)\n",
        "        self.actions.append(action)\n",
        "        self.rewards.append(reward)\n",
        "        self.values.append(value)\n",
        "        self.dones.append(done)\n",
        "        if log_prob is not None:\n",
        "            self.log_probs.append(log_prob)\n",
        "\n",
        "        # Update current buffer size\n",
        "        self.current_size = len(self.observations)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"\n",
        "        Convert stored experiences to numpy arrays for batch processing.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Arrays of observations, actions, rewards, values, dones, and log_probs\n",
        "        \"\"\"\n",
        "        return (np.array(self.observations),\n",
        "                np.array(self.actions),\n",
        "                np.array(self.rewards),\n",
        "                np.array(self.values),\n",
        "                np.array(self.dones),\n",
        "                np.array(self.log_probs) if self.log_probs else None)\n",
        "\n",
        "    def clear_memo(self):\n",
        "        \"\"\"Clear all stored experiences from the buffer.\"\"\"\n",
        "        self.observations = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.values = []\n",
        "        self.dones = []\n",
        "        self.log_probs = []\n",
        "        self.current_size = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Fw_8mgQAgPyi"
      },
      "outputs": [],
      "source": [
        "class PPOAgent:\n",
        "    \"\"\"\n",
        "    PPO (Proximal Policy Optimization) agent implementation.\n",
        "\n",
        "    This agent implements the PPO algorithm with the following features:\n",
        "    - Actor-Critic architecture with LSTM\n",
        "    - Discretized action space\n",
        "    - Clipped objective function\n",
        "    - Value function loss\n",
        "    - Entropy regularization\n",
        "    - Gradient clipping\n",
        "\n",
        "    Args:\n",
        "        obs_dim (int): Dimension of the observation space\n",
        "        action_dim (int): Dimension of the action space\n",
        "        batch_size (int): Size of experience replay buffer\n",
        "    \"\"\"\n",
        "    def __init__(self, obs_dim, action_dim, batch_size):\n",
        "        # PPO hyperparameters\n",
        "        self.LR_ACTOR = 3e-4          # Learning rate for actor network\n",
        "        self.LR_CRITIC = 3e-4         # Learning rate for critic network\n",
        "        self.GAMMA = 0.99             # Discount factor for future rewards\n",
        "        self.LAMBDA = 0.95            # GAE (Generalized Advantage Estimation) parameter\n",
        "        self.EPOCH = 10               # Number of optimization epochs\n",
        "        self.EPSILON_CLIP = 0.2       # PPO clipping parameter\n",
        "        self.ENT_COEF = 0.01          # Entropy coefficient for exploration\n",
        "        self.VF_COEF = 0.5            # Value function coefficient\n",
        "        self.MAX_GRAD_NORM = 0.5      # Maximum gradient norm for clipping\n",
        "\n",
        "        # Initialize neural networks\n",
        "        self.actor = Actor(obs_dim, action_dim).to(device)\n",
        "        self.old_actor = Actor(obs_dim, action_dim).to(device)  # Target network for stable training\n",
        "        self.critic = Critic(obs_dim).to(device)\n",
        "\n",
        "        # Initialize optimizers\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.LR_ACTOR)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.LR_CRITIC)\n",
        "\n",
        "        # Initialize replay buffer\n",
        "        self.replay_buffer = ReplayMemory(batch_size)\n",
        "\n",
        "        # Initialize LSTM states\n",
        "        self.policy_hidden_state = None\n",
        "        self.value_hidden_state = None\n",
        "\n",
        "        # Action space setup\n",
        "        self.action_tensor = torch.tensor(\n",
        "            [-1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1]\n",
        "        ).to(device)\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        \"\"\"\n",
        "        Select action using current policy network.\n",
        "\n",
        "        Args:\n",
        "            obs (np.ndarray): Current observation/state\n",
        "\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - actions (np.ndarray): Selected actions\n",
        "                - value (np.ndarray): Value estimate for current state\n",
        "                - log_probs (np.ndarray): Log probabilities of selected actions\n",
        "        \"\"\"\n",
        "        # Convert observation to tensor and add batch and sequence dimensions\n",
        "        obs = torch.FloatTensor(obs).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get action probabilities and value estimate\n",
        "            action_probs, self.policy_hidden_state = self.actor(obs, self.policy_hidden_state)\n",
        "            value, self.value_hidden_state = self.critic(obs, self.value_hidden_state)\n",
        "\n",
        "            action_probs = action_probs.squeeze()\n",
        "            actions = torch.zeros(self.action_dim)\n",
        "            log_probs = torch.zeros(self.action_dim)\n",
        "\n",
        "            # Sample actions for each dimension\n",
        "            for dim in range(self.action_dim):\n",
        "                dist = torch.distributions.Categorical(action_probs[dim])\n",
        "                action_idx = dist.sample()\n",
        "                actions[dim] = self.action_tensor[action_idx]\n",
        "                log_probs[dim] = dist.log_prob(action_idx)\n",
        "\n",
        "        return actions.numpy(), value.cpu().numpy(), log_probs.numpy()\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"\n",
        "        Update policy and value networks using PPO algorithm.\n",
        "\n",
        "        Returns:\n",
        "            dict: Training metrics including actor loss, critic loss, and entropy\n",
        "        \"\"\"\n",
        "        # Copy current policy to old policy\n",
        "        self.old_actor.load_state_dict(self.actor.state_dict())\n",
        "\n",
        "        # Get experiences from buffer\n",
        "        memo_observations, memo_actions, memo_rewards, memo_values, memo_dones, _ = self.replay_buffer.sample()\n",
        "\n",
        "        # Convert to tensors\n",
        "        observations = torch.FloatTensor(memo_observations).unsqueeze(1).to(device)\n",
        "        actions = torch.FloatTensor(memo_actions).to(device)\n",
        "        old_values = torch.FloatTensor(memo_values).to(device)\n",
        "        rewards = torch.FloatTensor(memo_rewards).to(device)\n",
        "        dones = torch.FloatTensor(memo_dones).to(device)\n",
        "\n",
        "        # Get old action probabilities\n",
        "        with torch.no_grad():\n",
        "            old_action_probs, _ = self.old_actor(observations)\n",
        "            old_action_probs = old_action_probs.squeeze(1)\n",
        "\n",
        "        # Optimization loop\n",
        "        for _ in range(self.EPOCH):\n",
        "            # Get current action probabilities and values\n",
        "            action_probs, _ = self.actor(observations)\n",
        "            action_probs = action_probs.squeeze(1)\n",
        "            current_values, _ = self.critic(observations)\n",
        "\n",
        "            # Calculate advantages\n",
        "            advantages = rewards - old_values.squeeze(-1)\n",
        "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "            # Calculate probability ratios\n",
        "            ratios = torch.exp(\n",
        "                torch.sum(\n",
        "                    torch.log(action_probs + 1e-10) - torch.log(old_action_probs + 1e-10),\n",
        "                    dim=-1\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Calculate surrogate losses\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.EPSILON_CLIP, 1+self.EPSILON_CLIP) * advantages\n",
        "\n",
        "            # Calculate final losses\n",
        "            actor_loss = -torch.min(surr1, surr2).mean()\n",
        "            critic_loss = F.mse_loss(current_values.squeeze(), rewards.unsqueeze(-1).squeeze())\n",
        "            entropy = -torch.mean(\n",
        "                torch.sum(-action_probs * torch.log(action_probs + 1e-10), dim=-1)\n",
        "            )\n",
        "\n",
        "            # Combined loss\n",
        "            loss = actor_loss + 0.5 * critic_loss - self.ENT_COEF * entropy\n",
        "\n",
        "            # Optimize\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip gradients\n",
        "            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.MAX_GRAD_NORM)\n",
        "            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.MAX_GRAD_NORM)\n",
        "\n",
        "            # Update networks\n",
        "            self.actor_optimizer.step()\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "        # Clear replay buffer\n",
        "        self.replay_buffer.clear_memo()\n",
        "\n",
        "        # Return training metrics\n",
        "        return {\n",
        "            'actor_loss': actor_loss.item(),\n",
        "            'critic_loss': critic_loss.item(),\n",
        "            'entropy': entropy.item()\n",
        "        }\n",
        "\n",
        "    def save_policy(self, path):\n",
        "        \"\"\"\n",
        "        Save the actor network to disk.\n",
        "\n",
        "        Args:\n",
        "            path (str): Path where to save the model\n",
        "        \"\"\"\n",
        "        torch.save(self.actor.state_dict(), path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Xlgv_0GtgdRO"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(env, agent, num_episodes=5):\n",
        "    \"\"\"\n",
        "    Evaluate the current policy without training.\n",
        "\n",
        "    Features:\n",
        "    - Tracks total rewards and success rate\n",
        "    - Monitors consecutive successful episodes\n",
        "    - Preserves LSTM states during evaluation\n",
        "    - Handles early termination\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): Evaluation environment\n",
        "        agent (PPOAgent): Agent to evaluate\n",
        "        num_episodes (int): Number of episodes to evaluate\n",
        "\n",
        "    Returns:\n",
        "        dict: Evaluation metrics including mean reward, success rate, and consecutive successes\n",
        "    \"\"\"\n",
        "    total_rewards = []\n",
        "    success_rate = []\n",
        "    episode_lengths = []\n",
        "    consecutive_successes = 0\n",
        "    max_consecutive_successes = 0\n",
        "\n",
        "    # Store original LSTM states\n",
        "    orig_policy_hidden = agent.policy_hidden_state\n",
        "    orig_value_hidden = agent.value_hidden_state\n",
        "\n",
        "    # Reset LSTM states for evaluation\n",
        "    agent.policy_hidden_state = None\n",
        "    agent.value_hidden_state = None\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        state = flatten_observation(state)\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "        success = False\n",
        "\n",
        "        while True:\n",
        "            # Get action from policy\n",
        "            action, _, _ = agent.get_action(state)\n",
        "\n",
        "            # Execute action in environment\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            next_state = flatten_observation(next_state)\n",
        "\n",
        "            # Update episode information\n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "\n",
        "            # Check for success\n",
        "            if info.get('is_success', False):\n",
        "                success = True\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            # Check termination conditions\n",
        "            if terminated or truncated or steps >= 200:\n",
        "                break\n",
        "\n",
        "        # Update consecutive success tracking\n",
        "        if success:\n",
        "            consecutive_successes += 1\n",
        "            max_consecutive_successes = max(max_consecutive_successes, consecutive_successes)\n",
        "        else:\n",
        "            consecutive_successes = 0\n",
        "\n",
        "        # Store episode results\n",
        "        total_rewards.append(episode_reward)\n",
        "        success_rate.append(float(success))\n",
        "        episode_lengths.append(steps)\n",
        "\n",
        "    # Restore original LSTM states\n",
        "    agent.policy_hidden_state = orig_policy_hidden\n",
        "    agent.value_hidden_state = orig_value_hidden\n",
        "\n",
        "    # Return comprehensive evaluation metrics\n",
        "    return {\n",
        "        'mean_reward': np.mean(total_rewards),\n",
        "        'std_reward': np.std(total_rewards),\n",
        "        'success_rate': np.mean(success_rate),\n",
        "        'mean_episode_length': np.mean(episode_lengths),\n",
        "        'max_consecutive_successes': max_consecutive_successes\n",
        "    }\n",
        "\n",
        "def flatten_observation(obs):\n",
        "    \"\"\"\n",
        "    Convert dictionary observation to flat array.\n",
        "\n",
        "    Args:\n",
        "        obs (dict): Dictionary containing observation values\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Flattened observation array\n",
        "    \"\"\"\n",
        "    return np.concatenate([obs[key].flatten() for key in sorted(obs.keys())])\n",
        "\n",
        "def plot_training_curves(log_dir, timestamp):\n",
        "    \"\"\"\n",
        "    Generate comprehensive training visualization from training data.\n",
        "\n",
        "    Args:\n",
        "        log_dir (str): Directory containing training logs\n",
        "        timestamp (str): Timestamp for file naming\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load data from tensorboard logs\n",
        "        from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "        import pandas as pd\n",
        "\n",
        "        event_acc = EventAccumulator(log_dir)\n",
        "        event_acc.Reload()\n",
        "\n",
        "        # Extract metrics from tensorboard logs\n",
        "        metrics_data = {}\n",
        "        for tag in event_acc.Tags()['scalars']:\n",
        "            events = event_acc.Scalars(tag)\n",
        "            metrics_data[tag] = {\n",
        "                'steps': [e.step for e in events],\n",
        "                'values': [e.value for e in events]\n",
        "            }\n",
        "\n",
        "        # Create main figure\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "        gs = plt.GridSpec(3, 2, figure=fig)\n",
        "\n",
        "        # Plot configurations\n",
        "        plot_configs = [\n",
        "            ('Episode_reward', 'Episode Rewards', gs[0, 0], 'Reward'),\n",
        "            ('Average_reward', 'Moving Average Reward', gs[0, 1], 'Average Reward'),\n",
        "            ('eval_success_rate', 'Success Rate', gs[1, 0], 'Success Rate (%)'),\n",
        "            ('eval_max_consecutive_successes', 'Max Consecutive Successes', gs[1, 1], 'Count'),\n",
        "            ('actor_loss', 'Actor Loss', gs[2, 0], 'Loss'),\n",
        "            ('critic_loss', 'Critic Loss', gs[2, 1], 'Loss')\n",
        "        ]\n",
        "\n",
        "        for metric_name, title, position, ylabel in plot_configs:\n",
        "            if metric_name in metrics_data:\n",
        "                ax = fig.add_subplot(position)\n",
        "                data = metrics_data[metric_name]\n",
        "                steps = data['steps']\n",
        "                values = data['values']\n",
        "\n",
        "                if len(values) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Convert to pandas Series for easier manipulation\n",
        "                series = pd.Series(values, index=steps)\n",
        "\n",
        "                # Plot raw data\n",
        "                ax.plot(steps, values, 'b-', alpha=0.3, label='Raw Data')\n",
        "\n",
        "                # Add moving average for smoothing\n",
        "                if len(values) > 5:\n",
        "                    window_size = min(10, len(values) // 5)\n",
        "                    rolling_mean = series.rolling(window=window_size, min_periods=1).mean()\n",
        "                    ax.plot(steps, rolling_mean, 'r-', linewidth=2,\n",
        "                           label=f'{window_size}-point Moving Average')\n",
        "\n",
        "                # Add trend line\n",
        "                if len(values) > 1:\n",
        "                    z = np.polyfit(steps, values, 1)\n",
        "                    p = np.poly1d(z)\n",
        "                    ax.plot(steps, p(steps), 'g--', alpha=0.8,\n",
        "                           label=f'Trend (slope: {z[0]:.2e})')\n",
        "\n",
        "                # Calculate statistics\n",
        "                stats = {\n",
        "                    'Mean': np.mean(values),\n",
        "                    'Std': np.std(values),\n",
        "                    'Max': np.max(values),\n",
        "                    'Min': np.min(values),\n",
        "                    'Latest': values[-1]\n",
        "                }\n",
        "\n",
        "                # Add statistics box\n",
        "                stats_text = '\\n'.join([f'{k}: {v:.3f}' for k, v in stats.items()])\n",
        "                ax.text(1.02, 0.5, stats_text,\n",
        "                       transform=ax.transAxes,\n",
        "                       bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'),\n",
        "                       verticalalignment='center')\n",
        "\n",
        "                # Customize plot\n",
        "                ax.set_title(title, pad=10, fontsize=12, fontweight='bold')\n",
        "                ax.set_xlabel('Episode', fontsize=10)\n",
        "                ax.set_ylabel(ylabel, fontsize=10)\n",
        "                ax.grid(True, alpha=0.3)\n",
        "                ax.legend(loc='upper left')\n",
        "\n",
        "                # Add minor grid\n",
        "                ax.minorticks_on()\n",
        "                ax.grid(True, which='minor', alpha=0.1)\n",
        "\n",
        "        # Add overall title\n",
        "        plt.suptitle('Training Progress Overview',\n",
        "                    fontsize=16, y=0.95, fontweight='bold')\n",
        "\n",
        "        # Adjust layout and save\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        save_path = os.path.join(log_dir, f'training_curves_{timestamp}.png')\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"\\nTraining curves saved to: {save_path}\")\n",
        "\n",
        "        plt.close()\n",
        "\n",
        "        # Create correlation plot only if we have enough aligned data\n",
        "        main_metrics = ['Episode_reward', 'eval_success_rate', 'eval_max_consecutive_successes']\n",
        "        aligned_data = {}\n",
        "\n",
        "        # Find common steps across all metrics\n",
        "        common_steps = None\n",
        "        for metric in main_metrics:\n",
        "            if metric in metrics_data:\n",
        "                steps = set(metrics_data[metric]['steps'])\n",
        "                if common_steps is None:\n",
        "                    common_steps = steps\n",
        "                else:\n",
        "                    common_steps = common_steps.intersection(steps)\n",
        "\n",
        "        if common_steps and len(common_steps) > 1:\n",
        "            common_steps = sorted(list(common_steps))\n",
        "\n",
        "            # Create aligned data\n",
        "            for metric in main_metrics:\n",
        "                if metric in metrics_data:\n",
        "                    steps = metrics_data[metric]['steps']\n",
        "                    values = metrics_data[metric]['values']\n",
        "                    step_to_value = dict(zip(steps, values))\n",
        "                    aligned_data[metric] = [step_to_value[step] for step in common_steps]\n",
        "\n",
        "            if len(aligned_data) > 1:\n",
        "                plt.figure(figsize=(12, 8))\n",
        "                df = pd.DataFrame(aligned_data, index=common_steps)\n",
        "                sns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0,\n",
        "                           vmin=-1, vmax=1, square=True)\n",
        "                plt.title('Metrics Correlation Matrix', pad=20)\n",
        "\n",
        "                corr_path = os.path.join(log_dir, f'correlation_matrix_{timestamp}.png')\n",
        "                plt.savefig(corr_path, dpi=300, bbox_inches='tight')\n",
        "                print(f\"Correlation matrix saved to: {corr_path}\")\n",
        "\n",
        "                plt.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error while plotting training curves: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cyC_fbBDgrlU"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    \"\"\"\n",
        "    Main training loop for PPO agent.\n",
        "\n",
        "    Features:\n",
        "    - Episodic training with early stopping\n",
        "    - Regular evaluation and model checkpointing\n",
        "    - Progress tracking and visualization\n",
        "    - Automatic model saving\n",
        "    - Exception handling and cleanup\n",
        "\n",
        "    Training Process:\n",
        "    1. Environment and agent initialization\n",
        "    2. Episode-based training loop\n",
        "    3. Regular policy updates\n",
        "    4. Periodic evaluation\n",
        "    5. Progress visualization\n",
        "    6. Model checkpointing\n",
        "    \"\"\"\n",
        "    # Initialize environments\n",
        "    env = gym.make('HandManipulateBlockDense-v1', max_episode_steps=100)\n",
        "    eval_env = gym.make('HandManipulateBlockDense-v1', max_episode_steps=100)\n",
        "\n",
        "    # Setup dimensions based on environment\n",
        "    obs_space = env.observation_space.spaces\n",
        "    obs_dim = sum(np.prod(space.shape) for space in obs_space.values())\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    # Training hyperparameters\n",
        "    NUM_EPISODES = 500        # Total number of training episodes\n",
        "    NUM_STEPS = 100             # Maximum steps per episode\n",
        "    UPDATE_INTERVAL = 20        # Steps between policy updates\n",
        "    BATCH_SIZE = 32            # Size of replay buffer\n",
        "    EVAL_INTERVAL = 2         # Episodes between evaluations\n",
        "\n",
        "    # Setup directories for saving\n",
        "    base_dir = os.getcwd()\n",
        "    model_dir = os.path.join(base_dir, 'models')\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Create unique timestamp for this training run\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "    log_dir = os.path.join(base_dir, 'ppo_logs', timestamp)\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize tensorboard writer\n",
        "    writer = SummaryWriter(log_dir)\n",
        "\n",
        "    # Initialize agent and tracking variables\n",
        "    agent = PPOAgent(obs_dim=obs_dim, action_dim=action_dim, batch_size=BATCH_SIZE)\n",
        "    reward_buffer = np.empty(shape=NUM_EPISODES)\n",
        "    best_reward = -float('inf')\n",
        "    best_consecutive_successes = 0\n",
        "    reward_window = deque(maxlen=100)\n",
        "\n",
        "    # Training metrics history\n",
        "    history = {\n",
        "        'episode_rewards': [],\n",
        "        'success_rates': [],\n",
        "        'consecutive_successes': [],\n",
        "        'episode_lengths': []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        for episode in range(NUM_EPISODES):\n",
        "            # Reset environment and agent state\n",
        "            state, _ = env.reset()\n",
        "            state = flatten_observation(state)\n",
        "            episode_reward = 0\n",
        "            steps_in_episode = 0\n",
        "\n",
        "            # Reset LSTM states at the start of each episode\n",
        "            agent.policy_hidden_state = None\n",
        "            agent.value_hidden_state = None\n",
        "\n",
        "            # Episode loop\n",
        "            for step in range(NUM_STEPS):\n",
        "                # Get action from policy\n",
        "                action, value, log_prob = agent.get_action(state)\n",
        "\n",
        "                # Execute action in environment\n",
        "                next_state, reward, terminated, truncated, info = env.step(action)\n",
        "                next_state = flatten_observation(next_state)\n",
        "\n",
        "                # Update episode information\n",
        "                episode_reward += reward\n",
        "                steps_in_episode = step + 1\n",
        "\n",
        "                # Store experience\n",
        "                done = terminated or truncated or step == NUM_STEPS - 1\n",
        "                agent.replay_buffer.add_memo(state, action, reward, value, done, log_prob)\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "                # Periodic policy update\n",
        "                if (step + 1) % UPDATE_INTERVAL == 0:\n",
        "                    update_info = agent.update()\n",
        "                    # Log training metrics\n",
        "                    writer.add_scalar(\"actor_loss\", update_info['actor_loss'], episode)\n",
        "                    writer.add_scalar(\"critic_loss\", update_info['critic_loss'], episode)\n",
        "                    writer.add_scalar(\"entropy\", update_info['entropy'], episode)\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            # Update tracking metrics\n",
        "            reward_buffer[episode] = episode_reward\n",
        "            reward_window.append(episode_reward)\n",
        "            avg_reward = np.mean(list(reward_window))\n",
        "\n",
        "            # Log episode metrics\n",
        "            writer.add_scalar(\"Episode_reward\", episode_reward, episode)\n",
        "            writer.add_scalar(\"Average_reward\", avg_reward, episode)\n",
        "            writer.add_scalar(\"Steps_in_episode\", steps_in_episode, episode)\n",
        "\n",
        "            # Periodic evaluation\n",
        "            if episode % EVAL_INTERVAL == 0:\n",
        "                eval_metrics = evaluate_policy(eval_env, agent)\n",
        "\n",
        "                # Log evaluation metrics\n",
        "                for metric_name, value in eval_metrics.items():\n",
        "                    writer.add_scalar(f\"eval_{metric_name}\", value, episode)\n",
        "                    history[metric_name] = history.get(metric_name, []) + [value]\n",
        "\n",
        "                print(f\"\\nEvaluation at episode {episode}:\")\n",
        "                print(f\"Mean reward: {eval_metrics['mean_reward']:.2f}\")\n",
        "                print(f\"Success rate: {eval_metrics['success_rate']:.2f}\")\n",
        "                print(f\"Max consecutive successes: {eval_metrics['max_consecutive_successes']}\")\n",
        "\n",
        "                # Save best model based on consecutive successes\n",
        "                if eval_metrics['max_consecutive_successes'] > best_consecutive_successes:\n",
        "                    best_consecutive_successes = eval_metrics['max_consecutive_successes']\n",
        "                    model_path = os.path.join(model_dir,\n",
        "                                            f'ppo_actor_best_consecutive_{timestamp}.pth')\n",
        "                    agent.save_policy(model_path)\n",
        "                    print(f\"New best consecutive successes: {best_consecutive_successes}!\")\n",
        "\n",
        "            # Save best model based on episode reward\n",
        "            if episode_reward > best_reward:\n",
        "                best_reward = episode_reward\n",
        "                model_path = os.path.join(model_dir, f'ppo_actor_best_reward_{timestamp}.pth')\n",
        "                agent.save_policy(model_path)\n",
        "                print(f\"New best reward: {best_reward:.2f}!\")\n",
        "\n",
        "            # Print episode summary\n",
        "            print(f\"\\nEpisode: {episode}\")\n",
        "            print(f\"Reward: {episode_reward:.2f}\")\n",
        "            print(f\"Average Reward: {avg_reward:.2f}\")\n",
        "            print(f\"Steps: {steps_in_episode}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            # Early stopping if we achieve consistent success\n",
        "            if best_consecutive_successes >= 50:  # Adjustable threshold\n",
        "                print(\"\\nReached target consecutive successes! Training complete.\")\n",
        "                break\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nTraining interrupted by user\")\n",
        "\n",
        "    finally:\n",
        "        # Cleanup and save final results\n",
        "        env.close()\n",
        "        eval_env.close()\n",
        "        writer.close()\n",
        "\n",
        "        # Save final model\n",
        "        final_model_path = os.path.join(model_dir, f'ppo_actor_final_{timestamp}.pth')\n",
        "        agent.save_policy(final_model_path)\n",
        "\n",
        "        # Save reward history\n",
        "        reward_path = os.path.join(base_dir, f'ppo_reward_{timestamp}.txt')\n",
        "        np.savetxt(reward_path, reward_buffer)\n",
        "\n",
        "        # Generate and save training curves\n",
        "        plot_training_curves(log_dir, timestamp)\n",
        "\n",
        "        print(f\"\\nTraining completed!\")\n",
        "        print(f\"Models saved in {model_dir}\")\n",
        "        print(f\"Logs saved in {log_dir}\")\n",
        "        print(f\"Rewards saved to {reward_path}\")\n",
        "        print(f\"Best consecutive successes achieved: {best_consecutive_successes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDv-bZ0mgs4u",
        "outputId": "237b8de1-9a4d-419b-caee-cda8057182c1"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
