{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rM1WQyD6fzDp",
    "outputId": "da2f4d95-3416-4833-8514-b7c8f4196008"
   },
   "outputs": [],
   "source": [
    "! pip install gymnasium\n",
    "! pip install gymnasium-robotics\n",
    "! pip install mujoco\n",
    "! pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loaner\\Desktop\\Study\\Reinforcement_Learning\\Code\\Rough\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Model (Using RSAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "from datetime import datetime\n",
    "import gymnasium_robotics\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set device for PyTorch (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, fc_hidden_dim=256, lstm_hidden_dim=128):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, fc_hidden_dim)\n",
    "        self.lstm_layer = nn.LSTM(fc_hidden_dim, lstm_hidden_dim, batch_first=True)\n",
    "        self.mean_layer = nn.Linear(lstm_hidden_dim, action_dim)\n",
    "        self.log_std_layer = nn.Linear(lstm_hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x, hidden_state=None):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if hidden_state is None:\n",
    "            x, hidden_state = self.lstm_layer(x)\n",
    "        else:\n",
    "            x, hidden_state = self.lstm_layer(x, hidden_state)\n",
    "        mean = torch.tanh(self.mean_layer(x))\n",
    "        log_std = torch.clamp(self.log_std_layer(x), -20, 2)\n",
    "        return mean, log_std, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, fc_hidden_dim=256, lstm_hidden_dim=128):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim + action_dim, fc_hidden_dim)\n",
    "        self.lstm_layer = nn.LSTM(fc_hidden_dim, lstm_hidden_dim, batch_first=True)\n",
    "        self.q_layer = nn.Linear(lstm_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, obs, action, hidden_state=None):\n",
    "        x = torch.cat([obs, action], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if hidden_state is None:\n",
    "            x, hidden_state = self.lstm_layer(x)\n",
    "        else:\n",
    "            x, hidden_state = self.lstm_layer(x, hidden_state)\n",
    "        q_value = self.q_layer(x)\n",
    "        return q_value, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, done):\n",
    "        self.buffer.append((obs, action, reward, next_obs, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        batch = [self.buffer[i] for i in idx]\n",
    "        obs, action, reward, next_obs, done = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(obs, dtype=torch.float32).to(device),\n",
    "            torch.tensor(action, dtype=torch.float32).to(device),\n",
    "            torch.tensor(reward, dtype=torch.float32).unsqueeze(1).to(device),\n",
    "            torch.tensor(next_obs, dtype=torch.float32).to(device),\n",
    "            torch.tensor(done, dtype=torch.float32).unsqueeze(1).to(device),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_q_loss(critic, target_critic, actor, obs, action, reward, next_obs, done, alpha, gamma):\n",
    "    with torch.no_grad():\n",
    "        next_mean, next_log_std, _ = actor(next_obs)\n",
    "        next_std = next_log_std.exp()\n",
    "        next_dist = torch.distributions.Normal(next_mean, next_std)\n",
    "        next_action = next_dist.rsample()\n",
    "        next_log_prob = next_dist.log_prob(next_action).sum(dim=-1, keepdim=True)\n",
    "        target_q_value, _ = target_critic(next_obs, next_action)\n",
    "        target_value = reward + gamma * (1 - done) * (target_q_value - alpha * next_log_prob)\n",
    "    current_q_value, _ = critic(obs, action)\n",
    "    return F.mse_loss(current_q_value, target_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_loss(actor, critic, obs, alpha):\n",
    "    mean, log_std, _ = actor(obs)\n",
    "    std = log_std.exp()\n",
    "    dist = torch.distributions.Normal(mean, std)\n",
    "    action = dist.rsample()\n",
    "    log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)\n",
    "    q_value, _ = critic(obs, action)\n",
    "    return (alpha * log_prob - q_value).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_observation(obs):\n",
    "    \"\"\"\n",
    "    Flatten dictionary-based observations into a single array.\n",
    "    \"\"\"\n",
    "    return np.concatenate([value.flatten() for value in obs.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(log_dir, timestamp):\n",
    "    \"\"\"\n",
    "    Generate comprehensive training visualization from training data.\n",
    "\n",
    "    Args:\n",
    "        log_dir (str): Directory containing training logs\n",
    "        timestamp (str): Timestamp for file naming\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "        import pandas as pd\n",
    "\n",
    "        # Load TensorBoard logs\n",
    "        event_acc = EventAccumulator(log_dir)\n",
    "        event_acc.Reload()\n",
    "\n",
    "        # Print available tags to debug missing data\n",
    "        print(f\"Available tags in logs: {event_acc.Tags()['scalars']}\")\n",
    "\n",
    "        # Extract metrics from logs\n",
    "        metrics_data = {}\n",
    "        for tag in event_acc.Tags()['scalars']:\n",
    "            events = event_acc.Scalars(tag)\n",
    "            metrics_data[tag] = {\n",
    "                'steps': [e.step for e in events],\n",
    "                'values': [e.value for e in events],\n",
    "            }\n",
    "\n",
    "        # Create main figure\n",
    "        fig = plt.figure(figsize=(30, 20))  # Adjust figure size if needed\n",
    "        gs = plt.GridSpec(3, 2, figure=fig)\n",
    "\n",
    "        # Plot configurations\n",
    "        plot_configs = [\n",
    "            ('Reward/Episode', 'Episode Rewards', gs[0, 0], 'Reward'),\n",
    "            ('Reward/Average', 'Average Reward', gs[0, 1], 'Reward'),\n",
    "            ('Loss/Actor', 'Actor Loss', gs[1, 0], 'Loss'),\n",
    "            ('Loss/Critic', 'Critic Loss', gs[1, 1], 'Loss'),\n",
    "            ('Loss/Alpha', 'Alpha Loss', gs[2, 0], 'Loss'),\n",
    "            ('Success/Consecutive', 'Consecutive Successes', gs[2, 1], 'Count'),\n",
    "        ]\n",
    "\n",
    "        for metric_name, title, position, ylabel in plot_configs:\n",
    "            if metric_name in metrics_data:\n",
    "                ax = fig.add_subplot(position)\n",
    "                data = metrics_data[metric_name]\n",
    "                steps = data['steps']\n",
    "                values = data['values']\n",
    "\n",
    "                if len(values) == 0:\n",
    "                    print(f\"No data for metric: {metric_name}\")\n",
    "                    continue\n",
    "\n",
    "                # Convert to pandas Series for easier manipulation\n",
    "                series = pd.Series(values, index=steps)\n",
    "\n",
    "                # Plot raw data\n",
    "                ax.plot(steps, values, 'b-', alpha=0.3, label='Raw Data')\n",
    "\n",
    "                # Add moving average for smoothing\n",
    "                if len(values) > 5:\n",
    "                    window_size = min(10, len(values) // 5)\n",
    "                    rolling_mean = series.rolling(window=window_size, min_periods=1).mean()\n",
    "                    ax.plot(steps, rolling_mean, 'r-', linewidth=2,\n",
    "                            label=f'{window_size}-point Moving Average')\n",
    "\n",
    "                # Add trend line\n",
    "                if len(values) > 1:\n",
    "                    z = np.polyfit(steps, values, 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    ax.plot(steps, p(steps), 'g--', alpha=0.8,\n",
    "                            label=f'Trend (slope: {z[0]:.2e})')\n",
    "\n",
    "                # Add statistics box\n",
    "                stats = {\n",
    "                    'Mean': np.mean(values),\n",
    "                    'Std': np.std(values),\n",
    "                    'Max': np.max(values),\n",
    "                    'Min': np.min(values),\n",
    "                    'Latest': values[-1]\n",
    "                }\n",
    "                stats_text = '\\n'.join([f'{k}: {v:.3f}' for k, v in stats.items()])\n",
    "                ax.text(1.02, 0.5, stats_text,\n",
    "                        transform=ax.transAxes,\n",
    "                        bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'),\n",
    "                        verticalalignment='center')\n",
    "\n",
    "                # Customize plot\n",
    "                ax.set_title(title, pad=10, fontsize=12, fontweight='bold')\n",
    "                ax.set_xlabel('Episode', fontsize=10)\n",
    "                ax.set_ylabel(ylabel, fontsize=10)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.legend(loc='upper left')\n",
    "\n",
    "                # Add minor grid\n",
    "                ax.minorticks_on()\n",
    "                ax.grid(True, which='minor', alpha=0.1)\n",
    "\n",
    "        # Add overall title\n",
    "        plt.suptitle('Training Progress Overview', fontsize=16, y=0.95, fontweight='bold')\n",
    "\n",
    "        # Adjust layout and save\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        save_path = os.path.join(log_dir, f'training_curves2_{timestamp}.png')\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nTraining curves saved to: {save_path}\")\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while plotting training curves: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rsac(env, num_episodes, batch_size, actor_lr, critic_lr, alpha_lr, gamma=0.99, tau=0.005):\n",
    "    # Flatten dictionary-based observations\n",
    "    obs_dim = sum(np.prod(space.shape) for space in env.observation_space.spaces.values())\n",
    "    action_dim = env.action_space.shape[0]\n",
    "\n",
    "    actor = Actor(obs_dim, action_dim).to(device)\n",
    "    critic = Critic(obs_dim, action_dim).to(device)\n",
    "    target_critic = Critic(obs_dim, action_dim).to(device)\n",
    "    target_critic.load_state_dict(critic.state_dict())\n",
    "\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "\n",
    "    alpha = torch.tensor(0.2, requires_grad=True, device=device)\n",
    "    alpha_optimizer = optim.Adam([alpha], lr=alpha_lr)\n",
    "\n",
    "    replay_buffer = ReplayMemory(capacity=100000)\n",
    "\n",
    "    # Visualization Setup\n",
    "    base_dir = os.getcwd()\n",
    "    model_dir = os.path.join(base_dir, 'models2')\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    log_dir = os.path.join(base_dir, 'rsac_baselogs', timestamp)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    reward_buffer = np.empty(shape=num_episodes)\n",
    "    best_reward = -float('inf')\n",
    "    reward_window = deque(maxlen=100)\n",
    "    success_count = 0\n",
    "    consecutive_successes = 0\n",
    "    max_consecutive_successes = 0\n",
    "\n",
    "    print(f\"Logging to TensorBoard at {log_dir}\")\n",
    "\n",
    "    try:\n",
    "        for episode in range(num_episodes):\n",
    "            obs_dict, _ = env.reset()\n",
    "            obs = torch.tensor(flatten_observation(obs_dict), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            episode_reward = 0\n",
    "            hidden_state_actor, hidden_state_critic = None, None\n",
    "            success = False\n",
    "            \n",
    "            prev_reward=0\n",
    "            for t in range(env._max_episode_steps):\n",
    "                mean, log_std, hidden_state_actor = actor(obs, hidden_state_actor)\n",
    "                std = log_std.exp()\n",
    "                dist = torch.distributions.Normal(mean, std)\n",
    "                action = dist.sample().squeeze().cpu().numpy()\n",
    "                next_obs_dict, reward_t, done, truncated, info = env.step(action)\n",
    "                reward=reward_t-prev_reward\n",
    "                prev_reward=reward\n",
    "                next_obs = torch.tensor(flatten_observation(next_obs_dict), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                replay_buffer.add(obs.squeeze().cpu().numpy(), action, reward, next_obs.squeeze().cpu().numpy(), done or truncated)\n",
    "                obs = next_obs\n",
    "                episode_reward += reward\n",
    "\n",
    "                if 'is_success' in info and info['is_success']:\n",
    "                    success = True\n",
    "\n",
    "                if len(replay_buffer.buffer) > batch_size:\n",
    "                    obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "                    # Critic Update\n",
    "                    critic_loss = soft_q_loss(critic, target_critic, actor, obs_batch, action_batch, reward_batch, next_obs_batch, done_batch, alpha, gamma)\n",
    "                    critic_optimizer.zero_grad()\n",
    "                    critic_loss.backward()\n",
    "                    critic_optimizer.step()\n",
    "\n",
    "                    # Actor Update\n",
    "                    actor_loss = policy_loss(actor, critic, obs_batch, alpha)\n",
    "                    actor_optimizer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    actor_optimizer.step()\n",
    "\n",
    "                    # Alpha Update\n",
    "                    alpha_loss = -(alpha * (torch.logsumexp(next_obs_batch, dim=-1) + 1)).mean()\n",
    "                    alpha_optimizer.zero_grad()\n",
    "                    alpha_loss.backward()\n",
    "                    alpha_optimizer.step()\n",
    "\n",
    "                    # Update Target Critic\n",
    "                    for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "                        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "                if done or truncated:\n",
    "                    break\n",
    "\n",
    "            # Track success and consecutive successes\n",
    "            if success:\n",
    "                consecutive_successes += 1\n",
    "                max_consecutive_successes = max(max_consecutive_successes, consecutive_successes)\n",
    "            else:\n",
    "                consecutive_successes = 0\n",
    "\n",
    "            success_count += int(success)\n",
    "\n",
    "            # Logging\n",
    "            reward_buffer[episode] = episode_reward\n",
    "            reward_window.append(episode_reward)\n",
    "            avg_reward = np.mean(list(reward_window))\n",
    "\n",
    "            writer.add_scalar(\"Reward/Episode\", episode_reward, episode)\n",
    "            writer.add_scalar(\"Reward/Average\", avg_reward, episode)\n",
    "            writer.add_scalar(\"Loss/Actor\", actor_loss.item(), episode)\n",
    "            writer.add_scalar(\"Loss/Critic\", critic_loss.item(), episode)\n",
    "            writer.add_scalar(\"Loss/Alpha\", alpha_loss.item(), episode)\n",
    "            writer.add_scalar(\"Success/Consecutive\", consecutive_successes, episode)\n",
    "\n",
    "            # Print Episode Summary\n",
    "            print(f\"\\nEpisode: {episode}\")\n",
    "            print(f\"Reward: {episode_reward:.2f}\")\n",
    "            print(f\"Average Reward: {avg_reward:.2f}\")\n",
    "            print(f\"Steps: {t + 1}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # Periodic Evaluation\n",
    "            if episode % 2 == 0:\n",
    "                print(f\"Evaluation at episode {episode}:\")\n",
    "                print(f\"Mean reward: {np.mean(reward_buffer[:episode+1]):.2f}\")\n",
    "                print(f\"Success rate: {success_count / (episode + 1):.2f}\")\n",
    "                print(f\"Max consecutive successes: {max_consecutive_successes}\")\n",
    "\n",
    "            # Save Best Model\n",
    "            if episode_reward > best_reward:\n",
    "                best_reward = episode_reward\n",
    "                torch.save(actor.state_dict(), os.path.join(model_dir, f\"actor_best_{timestamp}.pth\"))\n",
    "                print(f\"New best reward: {best_reward:.2f}!\")\n",
    "\n",
    "            # Save if early stopping condition met\n",
    "            if consecutive_successes >= 50:\n",
    "                print(\"\\nTarget consecutive successes achieved! Stopping early.\")\n",
    "                break\n",
    "\n",
    "        print(\"Training completed successfully!\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user. Performing cleanup...\")\n",
    "        torch.save(actor.state_dict(), os.path.join(model_dir, f\"actor_interrupted_{timestamp}.pth\"))\n",
    "        print(f\"Training interrupted! Partial model saved to {model_dir}\")\n",
    "\n",
    "    finally:\n",
    "        writer.close()\n",
    "        env.close()\n",
    "        plot_training_curves(log_dir, timestamp)\n",
    "        print(f\"Visualizations saved to {log_dir}\")\n",
    "\n",
    "\n",
    "# Train the RSAC model\n",
    "env = gym.make(\"HandManipulateBlockDense-v1\")\n",
    "train_rsac(env, num_episodes=5000, batch_size=64, actor_lr=1e-4, critic_lr=1e-4, alpha_lr=1e-4)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "idlf24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
