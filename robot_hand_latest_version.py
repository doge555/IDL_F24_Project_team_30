# -*- coding: utf-8 -*-
"""Robot_Hand

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ubDCtnB09bHBO0RsXz0IM6Rp4Yn36Mbk
"""

import numpy as np
import torch
from torch import nn
import torch.optim as optim
import gymnasium as gym
from tensorboardX import SummaryWriter
import os
from datetime import datetime
import gymnasium_robotics
import torch.nn.functional as F
from collections import deque
from torch.distributions.categorical import Categorical

gym.register_envs(gymnasium_robotics)

device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
print("Computing device: ", device)

class PermuteBlock(torch.nn.Module):
    def forward(self, x):
        return x.transpose(1, 2)

class Actor(nn.Module):
    def __init__(self, obs_dim, fc_hidden_dim=1024, lstm_hidden_dim=512, output_dim=(20, 11)):
        super(Actor, self).__init__()
        self.input_sequential = torch.nn.Sequential(
            PermuteBlock(),
            nn.BatchNorm1d(obs_dim),
            PermuteBlock(),
            nn.Linear(obs_dim, fc_hidden_dim),
            nn.ReLU()
        )
        self.lstm_layer = nn.LSTM(fc_hidden_dim, lstm_hidden_dim, batch_first=True)
        self.output_sequential = torch.nn.Sequential(
            nn.Linear(lstm_hidden_dim, output_dim[0]*output_dim[1]),
            nn.Unflatten(dim=-1, unflattened_size = output_dim),
            nn.Softmax(dim=-1)
        )
        self.action_tensor = torch.tensor([-1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1]).to(device)

    def forward(self, x, hidden_state=None):
        x = self.input_sequential(x)
        if hidden_state is None:
            x, hidden_state = self.lstm_layer(x)
        else:
            x, hidden_state = self.lstm_layer(x, hidden_state)
        action_dist = self.output_sequential(x)

        return action_dist, hidden_state

    def select_action(self, observation, hidden_state):
        logits, hidden_state = self.forward(observation, hidden_state)
        # max_idxs = action_prob.argmax(dim=-1)
        # actions = self.action_tensor[max_idxs]
        action_prob = Categorical(logits=logits)
        actions_idxs = action_prob.sample()
        actions = self.action_tensor[actions_idxs]
        return actions, hidden_state

class Critic(nn.Module):
    def __init__(self, value_input_dim, fc_hidden_dim=1024, lstm_hidden_dim=512, output_dim=1):
        super(Critic, self).__init__()
        self.input_sequential = torch.nn.Sequential(
            PermuteBlock(),
            nn.BatchNorm1d(value_input_dim),
            PermuteBlock(),
            nn.Linear(value_input_dim, fc_hidden_dim),
            nn.ReLU()
        )
        self.lstm_layer = nn.LSTM(fc_hidden_dim, lstm_hidden_dim, batch_first=True)
        self.output_sequential = torch.nn.Sequential(
            nn.Linear(lstm_hidden_dim, output_dim),
        )

    def forward(self, x, hidden_state=None):
        x = self.input_sequential(x)
        if hidden_state is None:
            x, hidden_state = self.lstm_layer(x)
        else:
            x, hidden_state = self.lstm_layer(x, hidden_state)
        values = self.output_sequential(x)

        return values, hidden_state

class ReplayMemory:
    def __init__(self, chunk_of_seq_size):
        self.policy_observation_cap = []
        self.value_observation_cap = []
        self.action_cap = []
        self.reward_cap = []
        self.value_cap = []
        self.done_cap = []
        self.chunk_of_seq_size = chunk_of_seq_size

    def add_memo(self, policy_observation, value_observation, action, reward, value, done):
        self.policy_observation_cap.append(policy_observation)
        self.value_observation_cap.append(value_observation)
        self.action_cap.append(action)
        self.reward_cap.append(reward)
        self.value_cap.append(value)
        self.done_cap.append(done)

    def sample(self):
        num_observation = len(self.policy_observation_cap)
        seq_start_points = np.arange(0, num_observation, self.chunk_of_seq_size)
        memory_indicies = np.arange(num_observation, dtype=np.int32)
        np.random.shuffle(memory_indicies)
        sequences = [memory_indicies[i:i + self.chunk_of_seq_size] for i in seq_start_points]  # Decrease the dimension

        return np.array(self.policy_observation_cap), \
            np.array(self.value_observation_cap), \
            np.array(self.action_cap), \
            np.array(self.reward_cap), \
            np.array(self.value_cap), \
            np.array(self.done_cap), \
            sequences

    def clear_memo(self):
        self.policy_observation_cap = []
        self.value_observation_cap = []
        self.action_cap = []
        self.reward_cap = []
        self.value_cap = []
        self.done_cap = []

class PPOAgent:
    def __init__(self, policy_dim, value_dim, batch_size):
        
        self.CRITIC_LOSS_WEIGHT = 0.5

        self.LR_ACTOR = 3e-4
        self.LR_CRITIC = 3e-4

        self.GAMMA = 0.99
        self.LAMBDA = 0.95
        self.EPOCH = 30
        self.EPSILON_CLIP = 0.2

        self.actor = Actor(policy_dim, fc_hidden_dim=1024, lstm_hidden_dim=512, output_dim=(20, 11)).to(device)
        self.old_actor = Actor(policy_dim, fc_hidden_dim=1024, lstm_hidden_dim=512, output_dim=(20, 11)).to(device)
        self.critic = Critic(value_dim, fc_hidden_dim=1024, lstm_hidden_dim=512, output_dim=1).to(device)
        
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.LR_ACTOR)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.LR_CRITIC)
        self.replay_buffer = ReplayMemory(batch_size)
        
        self.policy_hidden_state = None
        self.value_hidden_state = None
        self.action_tensor = torch.tensor([-1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1]).to(device)


    def get_action(self, policy_obs, value_obs):
        policy_obs = torch.FloatTensor(policy_obs).unsqueeze(1).to(device)
        value_obs = torch.FloatTensor(value_obs).unsqueeze(1).to(device)
        action, self.policy_hidden_state = self.actor.select_action(policy_obs, self.policy_hidden_state)
        value, self.value_hidden_state = self.critic.forward(value_obs, self.value_hidden_state)
        return action.detach().cpu().numpy()[:, 0, :], value.detach().cpu().numpy()[:, 0, :]

    def update(self):
        self.old_actor.load_state_dict(self.actor.state_dict())
        for epoch_i in range(self.EPOCH): 
            memo_policy_observations, memo_value_observations, memo_actions, memo_rewards, memo_values, memo_dones, sequences = self.replay_buffer.sample()
            # GAE caculation START
            T = len(memo_rewards)  # T sequences
            
            # memo_advantage in shape (sequences_size, batch_size, 1)
            memo_advantage = np.zeros_like(memo_rewards, dtype=np.float32)

            for t in range(T):
                discount = 1
                a_t = 0
                for k in range(t, T - 1):
                    a_t += discount * (
                            memo_rewards[k] + self.GAMMA * memo_values[k + 1] * (1 - int(memo_dones[k])) -
                            memo_values[k])
                    discount *= self.GAMMA * self.LAMBDA
                memo_advantage[t] = a_t
            # GAE caculation END

            with torch.no_grad():
                # memo_advantages_tensor in shape (batch_size, sequences_size, 1)
                memo_advantages_tensor = (torch.tensor(memo_advantage)).transpose(0, 1).to(device)
                memo_advantages_tensor = torch.cat([memo_advantages_tensor[:, chunk_of_seq, :] for chunk_of_seq in sequences], dim=1)

                #memo_values_tensor in shape (batch_size, sequences_size, 1)
                memo_values_tensor = (torch.tensor(memo_values)).transpose(0, 1).to(device)
                memo_values_tensor = torch.cat([memo_values_tensor[:, chunk_of_seq, :] for chunk_of_seq in sequences], dim=1)

            # memo_policy_observations_tensor in shape (batch_size, sequences_size, policy_dim)
            memo_policy_observations_tensor = (torch.FloatTensor(memo_policy_observations)).transpose(0, 1).to(device)
            memo_policy_observations_tensor = torch.cat([memo_policy_observations_tensor[:, chunk_of_seq, :] for chunk_of_seq in sequences], dim=1)

            # memo_value_observations_tensor in shape (batch_size, sequences_size, value_dim)
            memo_value_observations_tensor = (torch.FloatTensor(memo_value_observations)).transpose(0, 1).to(device)
            memo_value_observations_tensor = torch.cat([memo_value_observations_tensor[:, chunk_of_seq, :] for chunk_of_seq in sequences], dim=1)

            # memo_actions_tensor in shape (batch_size, sequences_size, 20)
            memo_actions_tensor = (torch.FloatTensor(memo_actions)).transpose(0, 1).to(device)
            memo_actions_tensor = torch.cat([memo_actions_tensor[:, chunk_of_seq, :] for chunk_of_seq in sequences], dim=1)

            # action probability according to old policy and action probability according to current policy caculation START
            with torch.no_grad():
                old_action_dist, _ = self.old_actor(memo_policy_observations_tensor)
            curr_action_dist, _ = self.actor(memo_policy_observations_tensor)

            old_action_prob = torch.gather(old_action_dist, -1, (torch.bucketize(memo_actions_tensor, self.action_tensor, right=True)-1).unsqueeze(-1)).squeeze(-1)
            curr_action_prob = torch.gather(curr_action_dist, -1, (torch.bucketize(memo_actions_tensor, self.action_tensor, right=True)-1).unsqueeze(-1)).squeeze(-1)
            # action probability according to old policy and action probability according to current policy caculation END

            # policy loss caculation START
            ratio = torch.sum(curr_action_prob / old_action_prob, dim=-1)
            surr1 = ratio * memo_advantages_tensor.squeeze(-1)
            surr2 = torch.clamp(ratio, 1 - self.EPSILON_CLIP, 1 + self.EPSILON_CLIP) * memo_advantages_tensor.squeeze(-1)
            actor_loss = -torch.min(surr1, surr2).mean()
            #  policy loss caculation END

            # value loss caculation START
            batch_returns = memo_advantages_tensor + memo_values_tensor
            batch_old_values, _ = self.critic(memo_value_observations_tensor)
            critic_loss = nn.MSELoss()(batch_old_values, batch_returns)
            # value loss caculation END

            # total loss START
            total_loss = actor_loss + self.CRITIC_LOSS_WEIGHT*critic_loss
            # total loss END

            # back proporgation through actor and critic networks START
            self.actor_optimizer.zero_grad()
            self.critic_optimizer.zero_grad()
            total_loss.backward()
            self.actor_optimizer.step()
            self.critic_optimizer.step()
            # back proporgation through actor and critic networks END

        # refresh the memory buffer 
        self.replay_buffer.clear_memo()
    
    def save_policy(self, path):
        torch.save(
            {"policy_state_dict": self.actor.state_dict(),
             "value_state_dict": self.critic.state_dict(),
            "policy_optimizer_state_dict": self.actor_optimizer.state_dict(),
            "value_optimizer_state_dict": self.critic_optimizer.state_dict(),
            }, path)

# def evaluate_policy(env, agent, num_episodes=5):
#     total_rewards = []
#     success_rate = []
#     episode_lengths = []

#     orig_policy_hidden = agent.policy_hidden_state
#     orig_value_hidden = agent.value_hidden_state
#     agent.policy_hidden_state = None
#     agent.value_hidden_state = None

#     for _ in range(num_episodes):
#         state, _ = env.reset()
#         state = flatten_observation(state)
#         episode_reward = 0
#         steps = 0
#         success = False

#         while True:
#             action, _, _ = agent.get_action(state)
#             next_state, reward, terminated, truncated, info = env.step(action)
#             next_state = flatten_observation(next_state)
#             episode_reward += reward
#             steps += 1

#             if info.get('is_success', False):
#                 success = True

#             state = next_state
#             if terminated or truncated or steps >= 200:
#                 break

#         total_rewards.append(episode_reward)
#         success_rate.append(float(success))
#         episode_lengths.append(steps)

#     agent.policy_hidden_state = orig_policy_hidden
#     agent.value_hidden_state = orig_value_hidden

#     return {
#         'mean_reward': np.mean(total_rewards),
#         'std_reward': np.std(total_rewards),
#         'success_rate': np.mean(success_rate),
#         'mean_episode_length': np.mean(episode_lengths)
#     }

# def flatten_observation(obs):
#     return np.concatenate([obs[key].flatten() for key in sorted(obs.keys())])

def noise_input(x, std):
    noise = np.random.normal(loc=0.0, scale=1.0, size=x.shape)
    return x + noise

def relative_target_orientation(curr, dest):
    op_norm_curr = np.sum(np.square(curr), axis = 1)
    inv_curr = curr * np.array([1, -1, -1, -1]) / op_norm_curr.reshape(-1,1)
    # apply quaternions multiplication alone all elements
    w1, x1, y1, z1 = inv_curr[:, 0], inv_curr[:, 1], inv_curr[:, 2], inv_curr[:, 3]
    w2, x2, y2, z2 = dest[:, 0], dest[:, 1], dest[:, 2], dest[:, 3]
    
    w = w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2
    x = w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2
    y = w1 * y2 - x1 * z2 + y1 * w2 + z1 * x2
    z = w1 * z2 + x1 * y2 - y1 * x2 + z1 * w2
    return np.column_stack((w, x, y, z))

def policy_input_converter(obs_dict):
    '''
        This function convert the raw observation from the environment to the input shape of a policy model input
        
        obs_dict = {
            achieved_goal:  Box(-inf, inf, (num_envs, 7), float64)
            desired_goal:   Box(-inf, inf, (num_envs, 7), float64)
            observation:    Box(-inf, inf, (num_envs, 61), float64)
        }
    '''
    policy_input = np.zeros( ( 6, 31 ) )           # 24 for fingertip, 7 for object pos and orient
    # Noisy Observation
    policy_input[:, :24]    = noise_input(obs_dict["observation"][:, :24], 0.01)                                                  # Angle for each finger joints
    policy_input[:, 24:27]  = noise_input(obs_dict["achieved_goal"][:, :3], 0.05)                                                 # Block position
    policy_input[:, 27:31]  = relative_target_orientation(obs_dict["achieved_goal"][:, 3:], obs_dict["desired_goal"][:, 3:])      # Relative target orientation
    return policy_input

def value_input_converter(obs_dict):
    '''
        This function convert the raw observation from the environment to the input shape of a policy model input
        
        obs_dict = {
            achieved_goal:  Box(-inf, inf, (num_envs, 7), float64)
            desired_goal:   Box(-inf, inf, (num_envs, 7), float64)
            observation:    Box(-inf, inf, (num_envs, 61), float64)
        }
    '''
    value_input = np.zeros( ( 6, 69 ) )
    # Observation
    value_input[:, : 54]    = obs_dict["observation"][:, :54]       # Hand joint angles and velocities (0-48), object velocity and angular velocity
    value_input[:, 54:61]   = obs_dict["achieved_goal"][:, :]       # Object position and orientation
    # Goal
    value_input[:, 61:65]   = obs_dict["desired_goal"][:, 3:]       # Relative
    value_input[:, 65:69]    = obs_dict["desired_goal"][:, 3:]       # Block target orientation (paper says 4dim, but in gym only 3 dim)
    return value_input

def make_env(gym_id):
    def thunk():
        env = gym.make(gym_id, render_mode = "rgb_array")
        return env
    return thunk


def train():
    # env = gym.make('HandManipulateBlockDense-v1', max_episode_steps=100)
    envs = gym.vector.SyncVectorEnv([make_env("HandManipulateBlockDense-v1") for _ in range(6)])
    BATCH_SIZE = 60
    NUM_EPISODES = 60
    NUM_STEP = 2*BATCH_SIZE
    best_average_reward = -100

    base_dir = os.getcwd()
    model_dir = os.path.join(base_dir, 'models')
    os.makedirs(model_dir, exist_ok=True)

    agent = PPOAgent(policy_dim=31, value_dim=69, batch_size=BATCH_SIZE)
    for episode in range(NUM_EPISODES):
        obs_dict, _ = envs.reset()
        done = False
        episode_reward = np.zeros(6,)
        reward_buffer = np.zeros((6, 1))
        for step_i in range(NUM_STEP):
            policy_obs = policy_input_converter(obs_dict)
            value_obs = value_input_converter(obs_dict)
            action, value = agent.get_action(policy_obs, value_obs)
            next_obs_dict, reward, done, _, _ = envs.step(action)
            episode_reward += reward
            done = True if step_i == NUM_STEP - 1 else False
            agent.replay_buffer.add_memo(policy_obs, value_obs, action, (reward - reward_buffer[:, -1]).reshape(-1, 1), value, done)
            reward_buffer = np.concatenate((reward_buffer, reward.reshape(-1, 1)), axis=1)
            obs_dict = next_obs_dict

        agent.update()
        
        if np.mean(episode_reward/NUM_STEP) > best_average_reward:
            best_average_reward = np.mean(episode_reward/NUM_STEP)
            model_path = os.path.join(model_dir, f'ppo_actor_best.pth')
            agent.save_policy(model_path)
            print(f"Best reward: {best_average_reward}!")

    # obs_dim = sum(np.prod(space.shape) for space in obs_space.values())
    # action_dim = env.action_space.shape[0]

    # NUM_EPISODES = 1000
    # NUM_STEPS = 100
    # UPDATE_INTERVAL = 20
    # BATCH_SIZE = 60
    # EVAL_INTERVAL = 10

    # base_dir = os.getcwd()
    # model_dir = os.path.join(base_dir, 'models')
    # os.makedirs(model_dir, exist_ok=True)

    # timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
    # log_dir = os.path.join(base_dir, 'ppo_logs', timestamp)
    # os.makedirs(log_dir, exist_ok=True)
    # writer = SummaryWriter(log_dir)

    # agent = PPOAgent(obs_dim=obs_dim, action_dim=action_dim, batch_size=BATCH_SIZE)
    # reward_buffer = np.empty(shape=NUM_EPISODES)
    # best_reward = -float('inf')
    # reward_window = deque(maxlen=100)

    # try:
    #     for episode in range(NUM_EPISODES):
    #         state, _ = env.reset()
    #         state = flatten_observation(state)
    #         episode_reward = 0
    #         steps_in_episode = 0

    #         agent.policy_hidden_state = None
    #         agent.value_hidden_state = None

    #         for step in range(NUM_STEPS):
    #             action, value, log_prob = agent.get_action(state)
    #             next_state, reward, terminated, truncated, info = env.step(action)
    #             next_state = flatten_observation(next_state)
    #             episode_reward += reward
    #             steps_in_episode = step + 1

    #             done = terminated or truncated or step == NUM_STEPS - 1
    #             agent.replay_buffer.add_memo(state, action, reward, value, done, log_prob)

    #             state = next_state

    #             if (step + 1) % UPDATE_INTERVAL == 0:
    #                 update_info = agent.update()
    #                 writer.add_scalar("actor_loss", update_info['actor_loss'], episode)
    #                 writer.add_scalar("critic_loss", update_info['critic_loss'], episode)
    #                 writer.add_scalar("entropy", update_info['entropy'], episode)

    #             if done:
    #                 break

    #         reward_buffer[episode] = episode_reward
    #         reward_window.append(episode_reward)
    #         avg_reward = np.mean(list(reward_window))

    #         writer.add_scalar("Episode_reward", episode_reward, episode)
    #         writer.add_scalar("Average_reward", avg_reward, episode)
    #         writer.add_scalar("Steps_in_episode", steps_in_episode, episode)

    #         if episode % EVAL_INTERVAL == 0:
    #             eval_metrics = evaluate_policy(eval_env, agent)
    #             for metric_name, value in eval_metrics.items():
    #                 writer.add_scalar(f"eval_{metric_name}", value, episode)
    #             print(f"Evaluation at episode {episode}:")
    #             print(f"Mean reward: {eval_metrics['mean_reward']:.2f}")
    #             print(f"Success rate: {eval_metrics['success_rate']:.2f}")

    #         if episode_reward > best_reward:
    #             best_reward = episode_reward
    #             model_path = os.path.join(model_dir, f'ppo_actor_best_{timestamp}.pth')
    #             agent.save_policy(model_path)
    #             print(f"New best reward: {best_reward:.2f}!")

    #         print(f"Episode: {episode}")
    #         print(f"Reward: {episode_reward:.2f}")
    #         print(f"Average Reward: {avg_reward:.2f}")
    #         print(f"Steps: {steps_in_episode}")
    #         print("-" * 50)

    # except KeyboardInterrupt:
    #         print("\nTraining interrupted by user")
    # finally:
    #         env.close()
    #         eval_env.close()
    #         writer.close()

    #         reward_path = os.path.join(base_dir, f'ppo_reward_{timestamp}.txt')
    #         np.savetxt(reward_path, reward_buffer)
    #         print(f"\nRewards saved to {reward_path}")

train()

